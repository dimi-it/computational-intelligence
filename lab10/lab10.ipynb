{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy, copy\n",
    "from random import random, randint\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    # print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\"Checks is elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def state_value(pos: State):\n",
    "    \"\"\"Evaluate state: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 10\n",
    "    elif win(pos.o):\n",
    "        return -100\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def possible_actions(state:State):\n",
    "        return [i for i in range(1, 9+1) if i not in state.x and i not in state.o]\n",
    "\n",
    "def random_action(state: State):\n",
    "    actions = possible_actions(state)\n",
    "    return actions[randint(0, len(actions) - 1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    q = defaultdict(float)\n",
    "    prev_state = None\n",
    "    prev_action = None\n",
    "    def __init__(self, epsilon = 0.80, learning_rate = 0.8, discount_factor = 0.9):\n",
    "        self.epsilon = epsilon                      \n",
    "        self.learning_rate = learning_rate          \n",
    "        self.discount_factor = discount_factor    \n",
    "    \n",
    "    def get_action(self, state:State, learn):\n",
    "        actions = possible_actions(state)\n",
    "        if random() < self.epsilon or not learn:\n",
    "            q_values = [self.q[(state, i)] for i in actions]\n",
    "            return actions[np.argmax(q_values)]\n",
    "        else:\n",
    "            return actions[randint(0, len(actions) - 1)]\n",
    "    \n",
    "    def get_maxQ(self, state:State):\n",
    "        return max(self.q[(state, action)] for action in possible_actions(state))\n",
    "    \n",
    "    @staticmethod\n",
    "    def invert_state(state: State):\n",
    "        return State(state.o, state.x)\n",
    "    \n",
    "    def move(self, state:State, learn=False):\n",
    "        action = self.get_action(state, learn)\n",
    "        state_moved = State((*state.x, action), (*state.o,))\n",
    "        if learn:\n",
    "            if self.prev_state is not None:\n",
    "                old_q = self.q[(self.prev_state, self.prev_action)]\n",
    "                max_q = self.get_maxQ(state)\n",
    "                reward = state_value(state_moved)\n",
    "                if reward == 0:\n",
    "                    reward = -1\n",
    "                self.q[(self.prev_state, self.prev_action)] = (1 - self.learning_rate) * old_q + self.learning_rate * (reward + self.discount_factor * max_q)\n",
    "                if reward > 0:  #terminal win(update also last move q)\n",
    "                    old_q = self.q[(state, action)]\n",
    "                    self.q[(state, action)] = (1 - self.learning_rate) * old_q + self.learning_rate * reward\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action\n",
    "        return state_moved\n",
    "    \n",
    "    def end_play(self):\n",
    "        self.prev_state = None\n",
    "        self.prev_action = None\n",
    "    \n",
    "    def loss(self, state):\n",
    "        old_q = self.q[(self.prev_state, self.prev_action)]\n",
    "        reward = state_value(state)\n",
    "        self.q[(self.prev_state, self.prev_action)] =  (1 - self.learning_rate) * old_q + self.learning_rate * reward\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def move(self, state:State):\n",
    "        action = random_action(state)\n",
    "        next_state = State(state.x, (*state.o, action))\n",
    "        return next_state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 1_000_000\n",
    "history = []\n",
    "history_step = 1000\n",
    "step_win = 0\n",
    "step_loss = 0\n",
    "q_agent = QLearner()\n",
    "rnd_agent = RandomPlayer()\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    q_agent.end_play()\n",
    "    state = State((), ())\n",
    "    while True:\n",
    "        state = q_agent.move(state, learn=True)\n",
    "        if win(state.x):\n",
    "            step_win += 1\n",
    "            break\n",
    "        if len(possible_actions(state)) == 0:\n",
    "            #draw\n",
    "            break\n",
    "        state = rnd_agent.move(state)\n",
    "        if win(state.o):\n",
    "            q_agent.loss(state)\n",
    "            step_loss += 1\n",
    "            break\n",
    "    if i != 0 and i%history_step == 0:\n",
    "        history.append(step_win / (step_win+step_loss))\n",
    "        step_win = 0\n",
    "        step_loss = 0\n",
    "        \n",
    "print(len(q_agent.q))\n",
    "plt.plot(history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 100_000\n",
    "history = []\n",
    "q_agent = QLearner()\n",
    "rnd_agent = RandomPlayer()\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    story = []\n",
    "    state = State((), ())\n",
    "    while True:\n",
    "        state = q_agent.move(state)\n",
    "        story.append(state)\n",
    "        if win(state.x):\n",
    "            history.append(1)\n",
    "            break\n",
    "        if len(possible_actions(state)) == 0:\n",
    "            break\n",
    "        state = rnd_agent.move(state)\n",
    "        story.append(state)\n",
    "        if win(state.o):\n",
    "            history.append(0)\n",
    "            break\n",
    "print(f\"Win rate: {statistics.mean(history)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
